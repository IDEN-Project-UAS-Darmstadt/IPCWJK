@article{Vock2016,
abstract = {Models for predicting the probability of experiencing various health outcomes or adverse events over a certain time frame (e.g., having a heart attack in the next 5years) based on individual patient characteristics are important tools for managing patient care. Electronic health data (EHD) are appealing sources of training data because they provide access to large amounts of rich individual-level data from present-day patient populations. However, because EHD are derived by extracting information from administrative and clinical databases, some fraction of subjects will not be under observation for the entire time frame over which one wants to make predictions; this loss to follow-up is often due to disenrollment from the health system. For subjects without complete follow-up, whether or not they experienced the adverse event is unknown, and in statistical terms the event time is said to be right-censored. Most machine learning approaches to the problem have been relatively ad hoc; for example, common approaches for handling observations in which the event status is unknown include (1) discarding those observations, (2) treating them as non-events, (3) splitting those observations into two observations: one where the event occurs and one where the event does not. In this paper, we present a general-purpose approach to account for right-censored outcomes using inverse probability of censoring weighting (IPCW). We illustrate how IPCW can easily be incorporated into a number of existing machine learning algorithms used to mine big health care data including Bayesian networks,},
author = {Vock, David M and Wolfson, Julian and Bandyopadhyay, Sunayan and Adomavicius, Gediminas and Johnson, Paul E and Vazquez-Benitez, Gabriela and O'Connor, Patrick J},
doi = {10.1016/j.jbi.2016.03.009},
journal = {Journal of Biomedical Informatics},
pages = {119--131},
publisher = {Elsevier BV},
title = {{Adapting machine learning techniques to censored time-to-event health record data A general-purpose approach using inverse probability of censoring weighting}},
volume = {61},
year = {2016}
}
@article{Ginestet2021,
author = {Ginestet, Gonzales Pablo and Kotalik, Ales and Vock, David M and Wolfson, Julian and Gabriel, Erin E},
doi = {10.1111/rssc.12448},
journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
pages = {51--65},
publisher = {Oxford University Press (OUP)},
title = {{Stacked Inverse Probability of Censoring Weighted Bagging A Case Study In the InfCareHIV Register}},
volume = {70},
year = {2021}
}
@article{Blanche2023,
author = {Blanche, Paul Fr{\'{e}}d{\'{e}}ric and Holt, Anders and Scheike, Thomas},
doi = {10.1007/s10985-022-09564-6},
file = {:C\:/Users/ajahn/Downloads/s10985-022-09564-6 (2).pdf:pdf},
issn = {1380-7870},
journal = {Lifetime Data Analysis},
number = {2},
pages = {441--482},
title = {{On logistic regression with right censored data, with or without competing risks, and its use for estimating treatment effects}},
volume = {29},
year = {2023}
}
@article{Reps2021,
abstract = {BACKGROUND Researchers developing prediction models are faced with numerous design choices that may impact model performance. One key decision is how to include patients who are lost to follow-up. In this paper we perform a large-scale empirical evaluation investigating the impact of this decision. In addition, we aim to provide guidelines for how to deal with loss to follow-up. METHODS We generate a partially synthetic dataset with complete follow-up and simulate loss to follow-up based either on random selection or on selection based on comorbidity. In addition to our synthetic data study we investigate 21 real-world data prediction problems. We compare four simple strategies for developing models when using a cohort design that encounters loss to follow-up. Three strategies employ a binary classifier with data that: (1) include all patients (including those lost to follow-up), (2) exclude all patients lost to follow-up or (3) only exclude patients lost to follow-up who do not have the outcome before being lost to follow-up. The fourth strategy uses a survival model with data that include all patients. We empirically evaluate the discrimination and calibration performance. RESULTS The partially synthetic data study results show that excluding patients who are lost to follow-up can introduce bias when loss to follow-up is common and does not occur at random. However, when loss to follow-up was completely at random, the choice of addressing it had negligible impact on model discrimination performance. Our empirical real-world data results showed that the four design choices investigated to deal with loss to follow-up resulted in comparable performance when the time-at-risk was 1-year but demonstrated differential bias when we looked into 3-year time-at-risk. Removing patients who are lost to follow-up before experiencing the outcome but keeping patients who are lost to follow-up after the outcome can bias a model and should be avoided. CONCLUSION Based on this study we therefore recommend (1) developing models using data that includes patients that are lost to follow-up and (2) evaluate the discrimination and calibration of models twice: on a test set including patients lost to follow-up and a test set excluding patients lost to follow-up.},
author = {Reps, Jenna M and Rijnbeek, Peter and Cuthbert, Alana and Ryan, Patrick B and Pratt, Nicole and Schuemie, Martijn},
doi = {10.1186/s12911-021-01408-x},
issn = {1472-6947},
journal = {BMC medical informatics and decision making},
number = {1},
pages = {43},
pmid = {33549087},
title = {{An empirical analysis of dealing with patients who are lost to follow-up when developing prognostic models using a cohort design.}},
volume = {21},
year = {2021}
}
@article{Kvamme2023,
abstract = {The Brier score is commonly used for evaluating probability predictions. In survival analysis, with right-censored observations of the event times, this score can be weighted by the inverse probability of censoring (IPCW) to retain its original interpretation. It is common practice to estimate the censoring distribution with the Kaplan-Meier estimator, even though it assumes that the censoring distribution is independent of the covariates. This paper discusses the general impact of the censoring estimates on the Brier score and shows that the estimation of the censoring distribution can be problematic. In particular, when the censoring times can be identified from the covariates, the IPCW score is no longer valid. For administratively censored data, where the potential censoring times are known for all individuals, we propose an alternative version of the Brier score. This administrative Brier score does not require estimation of the censoring distribution and is valid even if the censoring times can be identified from the covariates.},
archivePrefix = {arXiv},
arxivId = {1912.08581},
author = {Kvamme, H{\aa}vard and Borgan, {\O}rnulf},
eprint = {1912.08581},
journal = {Journal of Machine Learning Research},
pages = {1--26},
title = {{The Brier Score under Administrative Censoring: Problems and Solutions}},
volume = {24},
year = {2023}
}
@Manual{pec,
    title = {pec: Prediction Error Curves for Risk Prediction Models in
      Survival Analysis},
    author = {Thomas A. Gerds},
    year = {2023},
    note = {R package version 2023.04.12},
    url = {https://CRAN.R-project.org/package=pec},
  }
@inproceedings{xgboost,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}
@Article{mets1,
title = {The Liability Threshold Model for Censored Twin Data},
author = {Klaus K. Holst and Thomas H. Scheike and Jacob B. Hjelmborg},
year = {2016},
volume = {93},
pages = {324-335},
journal = {Computational Statistics and Data Analysis},
doi = {10.1016/j.csda.2015.01.014},
}

@Article{mets2,
title = {Estimating heritability for cause specific mortality based on twin studies},
author = {Thomas H. Scheike and Klaus K. Holst and Jacob B.Hjelmborg},
year = {2014},
volume = {20},
number = {2},
pages = {210-233},
journal = {Lifetime Data Analysis},
doi = {10.1007/s10985-013-9244-x},
}
@article{Perme2019,
abstract = {The Mann–Whitney test is a commonly used non-parametric alternative of the two-sample t-test. Despite its frequent use, it is only rarely accompanied with confidence intervals of an effect size. If reported, the effect size is usually measured with the difference of medians or the shift of the two distribution locations. Neither of these two measures directly coincides with the test statistic of the Mann–Whitney test, so the interpretation of the test results and the confidence intervals may be importantly different. In this paper, we focus on the probability that random variable X is lower than random variable Y. This measure is often referred to as the degree of overlap or the probabilistic index; it is in one-to-one relationship with the Mann–Whitney test statistic. The measure equals the area under the ROC curve. Several methods have been proposed for the construction of the confidence interval for this measure, and we review the most promising ones and explain their ideas. We study the properties of different variance estimators and small sample problems of confidence intervals construction. We identify scenarios in which the existing approaches yield inadequate coverage probabilities. We conclude that the DeLong variance estimator is a reliable option regardless of the scenario, but confidence intervals should be constructed using the logit scale to avoid values above 1 or below 0 and the poor coverage probability that follows. A correction is needed for the case when all values from one sample are smaller than the values of the other. We propose a method that improves the coverage probability also in these cases.},
author = {Perme, Maja Pohar and Manevski, Damjan},
doi = {10.1177/0962280218814556,},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {Mann–Whitney,area under ROC curve,confidence interval,effect size,probabilistic index,small sample size},
month = {dec},
number = {12},
pages = {3755--3768},
pmid = {30514179},
publisher = {SAGE Publications Ltd},
title = {{Confidence intervals for the Mann–Whitney test}},
volume = {28},
year = {2019}
}

@book{Efron2016,
abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. ‘Big data', ‘data science', and ‘machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
author = {Efron, Bradley and Hastie, Trevor},
booktitle = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
month = {jan},
pages = {1--475},
publisher = {Cambridge University Press},
title = {{Computer age statistical inference: Algorithms, evidence, and data science}},
year = {2016}
}
@Manual{survival-package,
title = {A Package for Survival Analysis in R},
author = {Terry M Therneau},
year = {2024},
note = {R package version 3.8-3},
}