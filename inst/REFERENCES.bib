@article{Vock2016,
abstract = {Models for predicting the probability of experiencing various health outcomes or adverse events over a certain time frame (e.g., having a heart attack in the next 5years) based on individual patient characteristics are important tools for managing patient care. Electronic health data (EHD) are appealing sources of training data because they provide access to large amounts of rich individual-level data from present-day patient populations. However, because EHD are derived by extracting information from administrative and clinical databases, some fraction of subjects will not be under observation for the entire time frame over which one wants to make predictions; this loss to follow-up is often due to disenrollment from the health system. For subjects without complete follow-up, whether or not they experienced the adverse event is unknown, and in statistical terms the event time is said to be right-censored. Most machine learning approaches to the problem have been relatively ad hoc; for example, common approaches for handling observations in which the event status is unknown include (1) discarding those observations, (2) treating them as non-events, (3) splitting those observations into two observations: one where the event occurs and one where the event does not. In this paper, we present a general-purpose approach to account for right-censored outcomes using inverse probability of censoring weighting (IPCW). We illustrate how IPCW can easily be incorporated into a number of existing machine learning algorithms used to mine big health care data including Bayesian networks,},
author = {Vock, David M and Wolfson, Julian and Bandyopadhyay, Sunayan and Adomavicius, Gediminas and Johnson, Paul E and Vazquez-Benitez, Gabriela and O'Connor, Patrick J},
doi = {10.1016/j.jbi.2016.03.009},
journal = {Journal of Biomedical Informatics},
pages = {119--131},
publisher = {Elsevier BV},
title = {{Adapting machine learning techniques to censored time-to-event health record data A general-purpose approach using inverse probability of censoring weighting}},
url = {https://doi.org/10.1016/j.jbi.2016.03.009 http://dx.doi.org/10.1016/j.jbi.2016.03.009},
volume = {61},
year = {2016}
}
@article{Ginestet2021,
author = {Ginestet, Gonzales Pablo and Kotalik, Ales and Vock, David M and Wolfson, Julian and Gabriel, Erin E},
doi = {10.1111/rssc.12448},
journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
pages = {51--65},
publisher = {Oxford University Press (OUP)},
title = {{Stacked Inverse Probability of Censoring Weighted Bagging A Case Study In the InfCareHIV Register}},
url = {https://academic.oup.com/jrsssc/article/70/1/51/7033916 http://dx.doi.org/10.1111/rssc.12448},
volume = {70},
year = {2021}
}
@article{Blanche2023,
author = {Blanche, Paul Fr{\'{e}}d{\'{e}}ric and Holt, Anders and Scheike, Thomas},
doi = {10.1007/s10985-022-09564-6},
file = {:C\:/Users/ajahn/Downloads/s10985-022-09564-6 (2).pdf:pdf},
issn = {1380-7870},
journal = {Lifetime Data Analysis},
number = {2},
pages = {441--482},
title = {{On logistic regression with right censored data, with or without competing risks, and its use for estimating treatment effects}},
volume = {29},
year = {2023}
}
@article{Reps2021,
abstract = {BACKGROUND Researchers developing prediction models are faced with numerous design choices that may impact model performance. One key decision is how to include patients who are lost to follow-up. In this paper we perform a large-scale empirical evaluation investigating the impact of this decision. In addition, we aim to provide guidelines for how to deal with loss to follow-up. METHODS We generate a partially synthetic dataset with complete follow-up and simulate loss to follow-up based either on random selection or on selection based on comorbidity. In addition to our synthetic data study we investigate 21 real-world data prediction problems. We compare four simple strategies for developing models when using a cohort design that encounters loss to follow-up. Three strategies employ a binary classifier with data that: (1) include all patients (including those lost to follow-up), (2) exclude all patients lost to follow-up or (3) only exclude patients lost to follow-up who do not have the outcome before being lost to follow-up. The fourth strategy uses a survival model with data that include all patients. We empirically evaluate the discrimination and calibration performance. RESULTS The partially synthetic data study results show that excluding patients who are lost to follow-up can introduce bias when loss to follow-up is common and does not occur at random. However, when loss to follow-up was completely at random, the choice of addressing it had negligible impact on model discrimination performance. Our empirical real-world data results showed that the four design choices investigated to deal with loss to follow-up resulted in comparable performance when the time-at-risk was 1-year but demonstrated differential bias when we looked into 3-year time-at-risk. Removing patients who are lost to follow-up before experiencing the outcome but keeping patients who are lost to follow-up after the outcome can bias a model and should be avoided. CONCLUSION Based on this study we therefore recommend (1) developing models using data that includes patients that are lost to follow-up and (2) evaluate the discrimination and calibration of models twice: on a test set including patients lost to follow-up and a test set excluding patients lost to follow-up.},
author = {Reps, Jenna M and Rijnbeek, Peter and Cuthbert, Alana and Ryan, Patrick B and Pratt, Nicole and Schuemie, Martijn},
doi = {10.1186/s12911-021-01408-x},
issn = {1472-6947},
journal = {BMC medical informatics and decision making},
number = {1},
pages = {43},
pmid = {33549087},
title = {{An empirical analysis of dealing with patients who are lost to follow-up when developing prognostic models using a cohort design.}},
volume = {21},
year = {2021}
}
@article{Kvamme2023,
abstract = {The Brier score is commonly used for evaluating probability predictions. In survival analysis, with right-censored observations of the event times, this score can be weighted by the inverse probability of censoring (IPCW) to retain its original interpretation. It is common practice to estimate the censoring distribution with the Kaplan-Meier estimator, even though it assumes that the censoring distribution is independent of the covariates. This paper discusses the general impact of the censoring estimates on the Brier score and shows that the estimation of the censoring distribution can be problematic. In particular, when the censoring times can be identified from the covariates, the IPCW score is no longer valid. For administratively censored data, where the potential censoring times are known for all individuals, we propose an alternative version of the Brier score. This administrative Brier score does not require estimation of the censoring distribution and is valid even if the censoring times can be identified from the covariates.},
archivePrefix = {arXiv},
arxivId = {1912.08581},
author = {Kvamme, H{\aa}vard and Borgan, {\O}rnulf},
eprint = {1912.08581},
journal = {Journal of Machine Learning Research},
pages = {1--26},
title = {{The Brier Score under Administrative Censoring: Problems and Solutions}},
url = {http://arxiv.org/abs/1912.08581},
volume = {24},
year = {2023}
}
@Manual{pec,
    title = {pec: Prediction Error Curves for Risk Prediction Models in
      Survival Analysis},
    author = {Thomas A. Gerds},
    year = {2023},
    note = {R package version 2023.04.12},
    url = {https://CRAN.R-project.org/package=pec},
  }
@inproceedings{xgboost,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}