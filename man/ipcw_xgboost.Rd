% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ipcw_xgboost.R
\name{ipcw_xgboost}
\alias{ipcw_xgboost}
\alias{ipcw_xgboost_default_grid}
\title{IPCW XGBoost Binary Classifier}
\usage{
ipcw_xgboost(
  data,
  tau,
  time_var = "t",
  status_var = "delta",
  verbose = 0,
  grid = ipcw_xgboost_default_grid(),
  nrounds = 100,
  early_stopping_rounds = 10,
  nfold = 3,
  nthread = 1
)

ipcw_xgboost_default_grid()
}
\arguments{
\item{data}{A data frame containing the survival data. Must include columns
for the observed time and event indicator.}

\item{tau}{Numeric scalar. The time horizon at which the survival
probability is to be estimated.}

\item{time_var}{Character. The name of the variable in \code{data}
representing the observed time to event or censoring.
Default is \code{"t"}.}

\item{status_var}{Character. The name of the variable in \code{data}
representing the event indicator (1 if event occurred, 0 if censored).
Default is \code{"delta"}.}

\item{verbose}{Integer. Verbosity level for XGBoost training and
cross-validation (default is 0).}

\item{grid}{Data frame. Grid of hyperparameters to test in cross-validation.
The default is the return of `ipcw_xgboost_default_grid()``.}

\item{nrounds}{Integer. Maximum number of boosting rounds for XGBoost
training and cross-validation (default is 100).}

\item{early_stopping_rounds}{Integer. Number of rounds with no improvement
to trigger early stopping during cross-validation (default is 10).}

\item{nfold}{Integer. Number of folds for cross-validation (default is 3).}

\item{nthread}{Integer. Number of threads to use for XGBoost training
(default is 1).}
}
\value{
An object of class \link{ipcwmodel}.
}
\description{
\loadmathjax
Fits a binary classification model using XGBoost with IPCW for
right-censored survival data. Hyperparameter tuning and Jackknife
model training are performed.
}
\details{
Training is performed using the \code{xgboost} package
\insertCite{xgboost}{IPCWJK} based on the \code{"binary:logistic"} objective.

Hyperparameter tuning is done using three (\code{nfold}) fold cross-validation
with a grid of parameters. The best parameters are selected based on the
minimum test log loss over 100 (\code{nrounds}) rounds with early stopping
(10 rounds, \code{early_stopping_rounds}).
Note that the tested hyperparameters are
based on our simulation and will not be useful for all datasets.

The tested hyperparameters include:
\itemize{
\item \code{booster}: \code{"gbtree"} or \code{"gblinear"}.
\item \code{eta}: Learning rate, tested as \code{1 / 10^(0:5)}.
\item For\code{booster="gblinear"}:
\itemize{
\item \code{max_depth}: Maximum depth of the tree, tested as \code{c(12, 6, 3, 1)}.
}
}

With the best parameters, the model is trained on the full dataset.
}
\section{Functions}{
\itemize{
\item \code{ipcw_xgboost_default_grid()}: Returns a default grid of hyperparameters.

}}
\examples{
library(survival)
tau <- 100
df <- veteran[, c("time", "status", "trt")]
newdata <- data.frame(trt = c(1, 2))

fit <- ipcw_xgboost(df,
  tau = tau, time_var = "time",
  status_var = "status"
)
predict(fit, newdata)
}
\references{
\insertAllCited{}
}
\seealso{
\code{\link[=ipcw_weights]{ipcw_weights()}} for the underlying implementation of the weights
and \link{IPCWJK} for more information.

Other IPCW models: 
\code{\link{ipcw_logistic_regression}()}
}
\concept{IPCW models}
